{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Train/Test Split and Bias and Variance\n",
    "\n",
    "_Authors: Joseph Nelson (DC), Kevin Markham (DC); Updated: B Rhodes (DC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Define error due to bias and error due to variance.\n",
    "- Identify the bias-variance trade-off.\n",
    "- Describe what overfitting and underfitting means in the context of model building.\n",
    "- Explain problems associated with over- and underfitting.\n",
    "- Grasp why train/test split is necessary.\n",
    "- Explore k-folds, LOOCV, and three split methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Bias and Variance Trade-Off](#bias-and-variance-trade-off)\n",
    "\t- [Bias? Variance?](#bias-variance)\n",
    "\t- [Exploring the Bias-Variance Trade-Off](#exploring-the-bias-variance-tradeoff)\n",
    "\t- [Brain and Body Weight Mammal Data Set](#brain-and-body-weight-mammal-dataset)\n",
    "\t- [Making a Prediction](#making-a-prediction)\n",
    "- [Making a Prediction From a Sample](#making-a-prediction-from-a-sample)\n",
    "\t- [Let's Try Something Completely Different](#lets-try-something-completely-different)\n",
    "- [Balancing Bias and Variance](#balancing-bias-and-variance)\n",
    "- [Train/Test Split](#train-test-split)\n",
    "\t- [Evaluation Procedure #1: Train and Test on the Entire Data Set (Do Not Do This)](#evaluation-procedure--train-and-test-on-the-entire-dataset-do-not-do-this)\n",
    "\t- [Problems With Training and Testing on the Same Data](#problems-with-training-and-testing-on-the-same-data)\n",
    "\t- [Evaluation Procedure #2: Train/Test Split](#evaluation-procedure--traintest-split)\n",
    "\t- [Comparing Test Performance With a Null Baseline](#comparing-test-performance-with-a-null-baseline)\n",
    "- [K-Folds Cross-Validation](#k-folds-cross-validation)\n",
    "\t- [Leave-One-Out Cross-Validation](#leave-one-out-cross-validation)\n",
    "\t- [Intro to Cross-Validation With the Boston Data](#intro-to-cross-validation-with-the-boston-data)\n",
    "- [Three-Way Data Split](#three-way-data-split)\n",
    "\t- [Additional Resources](#additional-resources)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-and-variance-trade-off\"></a>\n",
    "## Bias and Variance Trade-Off\n",
    "---\n",
    "\n",
    "The **bias-variance tradeoff** is widely used in machine learning as a conceptual way of comparing and contrasting different models. We only have a few methods that are able to compare all machine learning models. The others are more mathematical.\n",
    "\n",
    "**Bias** is error stemming from incorrect model assumptions.\n",
    "- Example: Assuming data is linear when it has a more complicated structure.\n",
    "\n",
    "**Variance** is error stemming from being overly sensitive from changes to the training data.\n",
    "- Example: Using the training set exactly (e.g. 1-NN) for a model results in a completely different model -- even if the training set differs only slightly.\n",
    "\n",
    "\n",
    "As model complexity **increases**:\n",
    "- Bias **decreases**. (The model can more accurately model complex structure in data.)\n",
    "- Variance **increases**. (The model identifies more complex structures, making it more sensitive to small changes in the training data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can't have a value taht is zero bias otherwise you don't need to use machine learning \n",
    "#as model complexity goes up bias decreases and variance increases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-variance\"></a>\n",
    "### Bias? Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Conceptual Definitions**\n",
    "- **Bias**: How close are predictions to the actual values?\n",
    "  - Roughly, whether or not our model aims on target.\n",
    "  - If the model cannot represent the data's structure, our predictions could be consistent, but will not be accurate.\n",
    "- **Variance**: How variable are our predictions?\n",
    "  - Roughly, whether or not our model is reliable.\n",
    "  - We will make slightly different predictions given slightly different training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/biasVsVarianceImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Visually, we are building a model where the bulls-eye is the goal.\n",
    "- Each individual hit is one prediction based on our model.\n",
    "- Critically, the success of our model (low variance, low bias) depends on the training data present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples:**\n",
    "\n",
    "- **Linear regression:** Low variance, High bias.\n",
    "    - If we train with a different subset of the training set, the model will be about the same. Hence, the model has low variance.\n",
    "    - The resulting model will predict the training points incorrectly (unless they happen to be perfectly linear). Hence, it has high bias.\n",
    "   \n",
    "\n",
    "- **Nearest neighbor:** High variance, Low bias.\n",
    "    - If we train with a different subset of the training set, the model will make predictions very differently. Hence, the model is highly variable.\n",
    "    - The resulting model will predict every training point perfectly. Hence, it has low bias.\n",
    "\n",
    "- **K-Nearest neightbor:** Med-high variance, Med-low bias.\n",
    "    - The model itself is more robust to outliers, so it will make more predictions the same than before. Hence, it has lower variance than 1-NN.\n",
    "    - The resulting model no longer predicts every point perfectly, since outliers will be mispredicted. So, the bias will be higher than before.\n",
    "\n",
    "\n",
    "See if you can figure out:\n",
    "\n",
    "- **High-order polynomial (as compared to linear regression)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expressing bias and variance mathematically:**\n",
    "\n",
    "It can be helpful understanding these terms by looking at how we can decompose the total error into them mathematically. (We will skip the derivations for now!)\n",
    "\n",
    "Let's define the error of our predictor as the expected value of our squared error. Note this error is not based on any particular fitted model, but on the family of potential models given a dataset (i.e. all fitted models made from all possible subsets of data).\n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = \\text{Bias}[\\hat{f}(x)]^2 + \\text{Var}[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "This states the expected error is based on only three components: **bias**, **variance**, and **irreducible error**.\n",
    "\n",
    "Breaking the bias and variance down further:\n",
    "\n",
    "$$\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x) - f(x)].$$\n",
    "\n",
    "- The bias is just the average expected distance between our predictor and actual values.\n",
    "\n",
    "$$\\text{Var}[\\hat{f}(x)] = E[\\hat{f}(x)^2] - E[\\hat{f}(x)]^2.$$\n",
    "\n",
    "- The variance is how much our predictions vary about the mean. ($E[\\hat{f}(x)]$ is our predictor's mean prediction.)\n",
    "\n",
    "- The irreducible error stems from noise in the problem itself.\n",
    "\n",
    "### MSE equation in plain english\n",
    "\n",
    "\n",
    "$$ \\text{MSE} = \\text{variance} + \\text{bias}^2 + \\text{irreducible error} $$\n",
    "\n",
    "\n",
    "\n",
    "**Some common questions:**\n",
    "\n",
    "From the math above, we can answer a few common questions:\n",
    "\n",
    "Can a model have high bias given one dataset and low bias for another?\n",
    "- Yes. If our data is linearly related, for example, it will have low bias on a linear model! However, in general across all datasets very few are accurately described with a linear model. So, in general we say a linear model has high bias and low variance.\n",
    "\n",
    "Is the MSE for a fitted linear regression the same thing as the bias?\n",
    "- It's close, but bias does not apply to a specific fitted model. Bias is the expected error of a model no matter what subset of the data it is fit on. This way, if we happen to get a lucky MSE fitting a model on a particular subset of our data, this does not mean we will have a low bias overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-the-bias-variance-tradeoff\"></a>\n",
    "### Exploring the Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:26.943161Z",
     "start_time": "2020-10-20T04:49:22.400259Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ce64074f609a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# model saving imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#import patsy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specific Imports\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "# model saving imports\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "#import patsy\n",
    "\n",
    "\n",
    "# Allow plots to appear in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"brain-and-body-weight-mammal-dataset\"></a>\n",
    "### Brain and Body Weight Mammal Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a [data set](http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt) of the average weight of the body (in kg) and the brain (in g) for 62 mammal species. We'll use this dataset to investigate bias vs. variance. Let's read it into Pandas and take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:26.986876Z",
     "start_time": "2020-10-20T04:49:26.946056Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'data/mammals.txt'\n",
    "cols = ['brain','body']\n",
    "mammals = pd.read_table(path, sep='\\t', names=cols, header=0)\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:27.010730Z",
     "start_time": "2020-10-20T04:49:26.989815Z"
    }
   },
   "outputs": [],
   "source": [
    "mammals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to focus on a smaller subset in which the body weight is less than 200 kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:27.023195Z",
     "start_time": "2020-10-20T04:49:27.016453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only keep rows in which the body weight is less than 200 kg.\n",
    "mammals = mammals[mammals.body < 200]\n",
    "mammals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We're now going to pretend that there are only 51 mammal species in existence. In other words, we are pretending that this is the entire data set of brain and body weights for **every known mammal species**.\n",
    "\n",
    "Let's create a scatterplot (using [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/)) to visualize the relationship between brain and body weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:27.276556Z",
     "start_time": "2020-10-20T04:49:27.026112Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, fit_reg=False);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There appears to be a relationship between brain and body weight for mammals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"making-a-prediction\"></a>\n",
    "### Making a Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-regression-quick-review\"></a>\n",
    "#### Linear Regression: A Quick Review\n",
    "\n",
    "![](./assets/linear-residuals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's pretend that a **new mammal species** is discovered. We measure the body weight of every member of this species we can find and calculate an **average body weight of 100 kgs**. We want to **predict the average brain weight** of this species (rather than measuring it directly). How might we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:27.603629Z",
     "start_time": "2020-10-20T04:49:27.280357Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We drew a straight line that appears to best capture the relationship between brain and body weight. So, we might predict that our new species has a brain weight of about 45 g, as that's the approximate y value when x=100.\n",
    "\n",
    "This is known as a \"linear model\" or a \"linear regression model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"making-a-prediction-from-a-sample\"></a>\n",
    "## Making a Prediction From a Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Earlier, we assumed that this dataset contained every known mammal species. That's very convenient, but **in the real world, all you ever have is a sample of data**. This may sound like a contentious statement, but the point of machine learning is to generalize from a sample to the population. If you already have data for the entire population, then you have no need for machine learning -- you can apply statistics directly and get optimal answers!\n",
    "\n",
    "Here, a more realistic situation would be to only have brain and body weights for (let's say) half of the 51 known mammals.\n",
    "\n",
    "When that new mammal species (with a body weight of 100 kg) is discovered, we still want to make an accurate prediction for its brain weight, but this task might be more difficult, as we don't have all of the data we would ideally like to have.\n",
    "\n",
    "Let's simulate this situation by assigning each of the 51 observations to **either universe 1 or universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:27.625720Z",
     "start_time": "2020-10-20T04:49:27.608398Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility.\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Randomly assign every observation to either universe 1 or universe 2.\n",
    "mammals['universe'] = np.random.randint(1, 3, len(mammals))\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Important:** We only live in one of the two universes. Both universes have 51 known mammal species, but each universe knows the brain and body weight for different species.\n",
    "\n",
    "We can now tell Seaborn to create two plots in which the left plot only uses the data from **universe 1** and the right plot only uses the data from **universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:28.183087Z",
     "start_time": "2020-10-20T04:49:27.634717Z"
    }
   },
   "outputs": [],
   "source": [
    "# col='universe' subsets the data by universe and creates two separate plots.\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe');\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The line looks pretty similar between the two plots, despite the fact that they used separate samples of data. In both cases, we would predict a brain weight of about 45 g.\n",
    "\n",
    "It's easier to see the degree of similarity by placing them on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:28.546768Z",
     "start_time": "2020-10-20T04:49:28.191197Z"
    }
   },
   "outputs": [],
   "source": [
    "# hue='universe' subsets the data by universe and creates a single plot.\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe');\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, what was the point of this exercise? This was a visual demonstration of a high-bias, low-variance model.\n",
    "\n",
    "- It's **high bias** because it doesn't fit the data particularly well.\n",
    "- It's **low variance** because it doesn't change much depending on which observations happen to be available in that universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"lets-try-something-completely-different\"></a>\n",
    "### Let's Try Something Completely Different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What would a **low bias, high variance** model look like? Let's try polynomial regression with an eighth-order polynomial.\n",
    "\n",
    "**Question**: is an 8th order model more or less complex than a linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:28.895823Z",
     "start_time": "2020-10-20T04:49:28.550403Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe', order=8);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- It's **low bias** because the models match the data effectively.\n",
    "- It's **high variance** because the models are widely different, depending on which observations happen to be available in that universe. (For a body weight of 100 kg, the brain weight prediction would be 40 kg in one universe and 0 kg in the other!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.190952Z",
     "start_time": "2020-10-20T04:49:28.899369Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe', order=8);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"balancing-bias-and-variance\"></a>\n",
    "## Balancing Bias and Variance\n",
    "Can we find a middle ground?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perhaps we can create a model that has **less bias than the linear model** and **less variance than the eighth order polynomial**?\n",
    "\n",
    "Let's try a second order polynomial instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.522128Z",
     "start_time": "2020-10-20T04:49:29.192563Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe', order=2);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.814395Z",
     "start_time": "2020-10-20T04:49:29.524971Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe', order=2);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This seems better. In both the left and right plots, **it fits the data well, but not too well**.\n",
    "\n",
    "This is the essence of the **bias-variance trade-off**: You are seeking a model that appropriately balances bias and variance and thus will generalize to new data (known as \"out-of-sample\" data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want a model that best balances bias and variance. It\n",
    "should match our training data well (moderate bias) yet be low variance for out-of-sample data (moderate variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Training error as a function of complexity.\n",
    "- **Question:** Why do we even care about variance if we know we can generate a more accurate model with higher complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we obtain a zero-bias, zero-variance model?\n",
    "\n",
    "No! If there is any noise in the data-generating process, then a zero-variance model would not be learning from the data. Additionally, a model only has zero bias if the true relationship between the target and the features is hard-coded into it. If that were the case, you wouldn't be doing machine learning -- it would be similar to trying to predict today's temperature by using today's temperature!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"train-test-split\"></a>\n",
    "## Train-test-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the lab, we will look at three evaluation procedures for predicting model out-of-sample accuracy:\n",
    "\n",
    "1. **Train on the entire dataset** should never be done to estimate model accuracy on out-of-sample data! After all, training error can be made arbitrarily small or large. You might train on the entire dataset as the very last step when a model is chosen, hoping to make the final model as accurate as possible. Or, you could use this to estimate the degree of overfitting.\n",
    "2. **Train-test-split** is useful if cross-validation is not practical (e.g. it takes too long to train). It is also useful for computing a quick confusion matrix. You could also use this as a final step after the model is finalized (often called evaluating the model against a **validation set**).\n",
    "3. **Cross-validation** is the gold standard for estimating accuracy and comparing accuracy across models.\n",
    "4. **Three-way split** combines cross-validation and the train-test-split. It takes an initial split to be used as a final validation set, then uses cross-validation on the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run into a problem when powerful models can perfectly fit the data on which they are trained. These models are **low bias** and **high variance**. However, we can't observe the variance of a model directly, because we only know how it fits the data we have rather than all potential samples.\n",
    "\n",
    "**Solution:** Create a procedure that **estimates** how well a model is likely to perform on out-of-sample data and use that to choose between models.\n",
    "\n",
    "- Before, we have been splitting the data into a **single training group** and a **single test group**.\n",
    "\n",
    "- Now, to estimate how well the model is likely to perform on out-of-sample data, we will create **many training groups** and **many test groups** then fit **many models**.\n",
    "\n",
    "**Note:** These procedures can be used with **any machine learning model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The Holdout Method: Train/Test Split**\n",
    "- **Training set**: Used to train the classifier.\n",
    "- **Testing set**: Used to estimate the error rate of the trained classifier.\n",
    "- **Advantages**: Fast, simple, computationally inexpensive.\n",
    "- **Disadvantages** Eliminates data, imperfectly splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"evaluation-procedure--train-and-test-on-the-entire-dataset-do-not-do-this\"></a>\n",
    "### Evaluation Procedure #1: Train and Test on the Entire Data Set (Do Not Do This)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Train the model on the **entire data set**.\n",
    "2. Test the model on the **same data set** and evaluate how well we did by comparing the **predicted** response values with the **true** response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: While the warning says do not do this. When might we want to train using the entire dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Boston data.\n",
    "\n",
    "*Note*: `sklearn` provides a number of \"toy\" datasets, the Boston housing data is just one example. You can see more here: `sklearn` [toy data sets](https://scikit-learn.org/stable/datasets/index.html#toy-datasets). These are useful to practice with since there are many tutorials and examples on the internet to compare your results against or to use for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.829377Z",
     "start_time": "2020-10-20T04:49:29.818918Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.838349Z",
     "start_time": "2020-10-20T04:49:29.832773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X and y variable to stores the feature matrix and response from the Boston data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.852586Z",
     "start_time": "2020-10-20T04:49:29.843223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for both parts of data; don't forget to assign column names.\n",
    "boston_data_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "target_ser = pd.DataFrame(boston.target, columns=['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate y and X, then overwrite the Boston variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.861189Z",
     "start_time": "2020-10-20T04:49:29.854924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate \n",
    "boston_df = pd.concat([boston_data_df, target_ser], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.930198Z",
     "start_time": "2020-10-20T04:49:29.865390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform basic EDA to make sure the data are in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:29.941421Z",
     "start_time": "2020-10-20T04:49:29.932438Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.010290Z",
     "start_time": "2020-10-20T04:49:29.944554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.4 KB\n"
     ]
    }
   ],
   "source": [
    "boston_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.092660Z",
     "start_time": "2020-10-20T04:49:30.013637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a feature matrix (X) and response (y)  for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.107735Z",
     "start_time": "2020-10-20T04:49:30.097648Z"
    }
   },
   "outputs": [],
   "source": [
    "# create feature matrix (X)\n",
    "# X is out feature values so we want all of the columns except MEDV\n",
    "feature_cols = boston_df.columns.drop(['MEDV'])\n",
    "X = boston_df[feature_cols]\n",
    "\n",
    "# create response vector (y)\n",
    "y = boston_df['MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import linear regression, instantiate, fit, and preview predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.148304Z",
     "start_time": "2020-10-20T04:49:30.131710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.00384338, 25.02556238, 30.56759672, 28.60703649, 27.94352423,\n",
       "       25.25628446, 23.00180827, 19.53598843, 11.52363685, 18.92026211])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model.\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train the model on the entire data set.\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Predict the response values for the observations in X (\"test the model\").\n",
    "# only show first 10 predictions\n",
    "lr.predict(X)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the predicted response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.169178Z",
     "start_time": "2020-10-20T04:49:30.161779Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To evaluate a model, we also need an **evaluation metric:**\n",
    "\n",
    "- A numeric calculation used to **quantify** the performance of a model.\n",
    "- The appropriate metric depends on the **goals** of your problem.\n",
    "\n",
    "The most common choices for regression problems are:\n",
    "\n",
    "- **R-squared**: The percentage of variation explained by the model (a \"reward function,\" as higher is better).\n",
    "- **Mean squared error**: The average squared distance between the prediction and the correct answer (a \"loss function,\" as lower is better).\n",
    "\n",
    "In this case, we'll use mean squared error because it is more interpretable in a predictive context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute mean squared error using a function from `metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.180752Z",
     "start_time": "2020-10-20T04:49:30.173137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 21.894831181729202\n"
     ]
    }
   ],
   "source": [
    "mse = metrics.mean_squared_error(y, y_pred)\n",
    "print(f'MSE = {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What units are this in?\n",
    "\n",
    "This is known as the **training mean squared error** because we are evaluating the model based on the same data we used to train the model.\n",
    "\n",
    "We can also calculate the **RMSE** or root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.192953Z",
     "start_time": "2020-10-20T04:49:30.187850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 4.679191295697281\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE = {np.sqrt(mse)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What are the units for RMSE in this problem?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#dollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problems-with-training-and-testing-on-the-same-data\"></a>\n",
    "### Problems With Training and Testing on the Same Data\n",
    "\n",
    "- Our goal is to estimate likely performance of a model on **out-of-sample data**.\n",
    "- But, maximizing the training mean squared error rewards **overly complex models** that won't necessarily generalize.\n",
    "- Unnecessarily complex models **overfit** the training data.\n",
    "    - They will do well when tested using the in-sample data.\n",
    "    - They may do poorly with out-of-sample data.\n",
    "    - They learn the \"noise\" in the data rather than the \"signal.\"\n",
    "    - From Quora: [What is an intuitive explanation of overfitting?](http://www.quora.com/What-is-an-intuitive-explanation-of-overfitting/answer/Jessica-Su)\n",
    "\n",
    "**Thus, the training MSE is not a good estimate of the out-of-sample MSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-procedure--traintest-split\"></a>\n",
    "### Evaluation procedure #2: Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Split the data set into two pieces: a **training set** and a **testing set**.\n",
    "2. Train the model on the **training set**.\n",
    "3. Test the model on the **testing set** and evaluate how well we did.\n",
    "\n",
    "Often a good rule-of-thumb is 70% training & 30% test, but this can vary based on the size of your dataset. For example, with a small dataset you would need to use as much training data as possible (in return, your test accuracy will be more variable).\n",
    "\n",
    "*Note: sklearn's `train_test_split()` method defaults to a 75/25 split for training vs testing data.*\n",
    "\n",
    "What does this accomplish?\n",
    "\n",
    "- Models can be trained and tested on **different data** (We treat testing data like out-of-sample data).\n",
    "- Response values are known for the testing set and thus **predictions can be evaluated**.\n",
    "\n",
    "This is known as the **testing mean squared error** because we are evaluating the model on an independent \"test set\" that was not used during model training.\n",
    "\n",
    "**The testing MSE is a better estimate of out-of-sample performance than the training MSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Understanding the `train_test_split` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.208118Z",
     "start_time": "2020-10-20T04:49:30.196683Z"
    }
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.216953Z",
     "start_time": "2020-10-20T04:49:30.211300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before and after splitting:\n",
      "Before split: (506, 13)\n",
      "Train split: (379, 13)\n",
      "Test split: (127, 13)\n"
     ]
    }
   ],
   "source": [
    "# Before splitting\n",
    "print(f'Features before and after splitting:')\n",
    "print(f'Before split: {X.shape}')\n",
    "\n",
    "# After splitting\n",
    "print(f'Train split: {X_train.shape}')\n",
    "print(f'Test split: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.227720Z",
     "start_time": "2020-10-20T04:49:30.221550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train proportion: 0.75\n",
      "Test proportion: 0.25\n"
     ]
    }
   ],
   "source": [
    "# spot check the size of our train and test data.\n",
    "print(f'Train proportion: {round(X_train.shape[0]/X.shape[0],2)}')\n",
    "print(f'Test proportion: {round(X_test.shape[0]/X.shape[0],2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.240378Z",
     "start_time": "2020-10-20T04:49:30.232521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before and after splitting:\n",
      "Before split: (506,)\n",
      "Train split: (379,)\n",
      "Test split: (127,)\n"
     ]
    }
   ],
   "source": [
    "# Recall that (1,) is a tuple. \n",
    "# The trailing comma distinguishes it as being a tuple, not an integer.\n",
    "\n",
    "# Before splitting\n",
    "print(f'Features before and after splitting:')\n",
    "print(f'Before split: {y.shape}')\n",
    "\n",
    "# After splitting\n",
    "print(f'Train split: {y_train.shape}')\n",
    "print(f'Test split: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.249983Z",
     "start_time": "2020-10-20T04:49:30.244613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train proportion: 0.75\n",
      "Test proportion: 0.25\n"
     ]
    }
   ],
   "source": [
    "# spot check the size of our train and test data.\n",
    "print(f'Train proportion: {round(y_train.shape[0]/y.shape[0],2)}')\n",
    "print(f'Test proportion: {round(y_test.shape[0]/y.shape[0],2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.271552Z",
     "start_time": "2020-10-20T04:49:30.252941Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>28.65580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597</td>\n",
       "      <td>5.155</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5894</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>210.97</td>\n",
       "      <td>20.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.65660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.871</td>\n",
       "      <td>6.122</td>\n",
       "      <td>97.3</td>\n",
       "      <td>1.6180</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>372.80</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.33983</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.431</td>\n",
       "      <td>6.108</td>\n",
       "      <td>34.9</td>\n",
       "      <td>8.0555</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>390.18</td>\n",
       "      <td>9.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.35233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.624</td>\n",
       "      <td>6.454</td>\n",
       "      <td>98.4</td>\n",
       "      <td>1.8498</td>\n",
       "      <td>4.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>394.08</td>\n",
       "      <td>14.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2.36862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.871</td>\n",
       "      <td>4.926</td>\n",
       "      <td>95.7</td>\n",
       "      <td>1.4608</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>391.71</td>\n",
       "      <td>29.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "413  28.65580   0.0  18.10   0.0  0.597  5.155  100.0  1.5894  24.0  666.0   \n",
       "150   1.65660   0.0  19.58   0.0  0.871  6.122   97.3  1.6180   5.0  403.0   \n",
       "246   0.33983  22.0   5.86   0.0  0.431  6.108   34.9  8.0555   7.0  330.0   \n",
       "137   0.35233   0.0  21.89   0.0  0.624  6.454   98.4  1.8498   4.0  437.0   \n",
       "147   2.36862   0.0  19.58   0.0  0.871  4.926   95.7  1.4608   5.0  403.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "413     20.2  210.97  20.08  \n",
       "150     14.7  372.80  14.10  \n",
       "246     19.1  390.18   9.16  \n",
       "137     21.2  394.08  14.59  \n",
       "147     14.7  391.71  29.53  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of X_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.287140Z",
     "start_time": "2020-10-20T04:49:30.279681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413    16.3\n",
       "150    21.5\n",
       "246    24.3\n",
       "137    17.1\n",
       "147    14.6\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of y_train\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_test_split](./assets/train_test_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Understanding the `random_state` Parameter\n",
    "\n",
    "The `random_state` is a pseudo-random number that allows us to reproduce our results every time we run them. However, it makes it impossible to predict what are exact results will be if we chose a new `random_state`.\n",
    "\n",
    "`random_state` is very useful for testing that your model was made correctly since it provides you with the same split each time. However, make sure you remove it if you are testing for model variability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.316484Z",
     "start_time": "2020-10-20T04:49:30.290945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.04544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>6.144</td>\n",
       "      <td>32.2</td>\n",
       "      <td>5.8736</td>\n",
       "      <td>4.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>16.9</td>\n",
       "      <td>368.57</td>\n",
       "      <td>9.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM   ZN  INDUS  CHAS   NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "330  0.04544  0.0   3.24   0.0  0.46  6.144  32.2  5.8736  4.0  430.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "330     16.9  368.57   9.09  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WITHOUT a random_state parameter:\n",
    "#  (If you run this code several times, you get different results!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Print the first element of each object.\n",
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.346135Z",
     "start_time": "2020-10-20T04:49:30.322900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRIM    ZN  INDUS  CHAS    NOX     RM  AGE     DIS  RAD    TAX  \\\n",
      "251  0.21409  22.0   5.86   0.0  0.431  6.438  8.9  7.3967  7.0  330.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  \n",
      "251     19.1  377.07   3.59  \n",
      "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "191  0.06911  45.0   3.44   0.0  0.437  6.739  30.8  6.4798  5.0  398.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  \n",
      "191     15.2  389.71   4.69  \n",
      "251    24.8\n",
      "Name: MEDV, dtype: float64\n",
      "191    30.5\n",
      "Name: MEDV, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# WITH a random_state parameter:\n",
    "#  (Same split every time! Note you can change the random state to any integer.)\n",
    "\n",
    "# Fill in your own value for random_state, i.e. replace the ? with any integer you want.\n",
    "# Try to pick an integer different than the instructor\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 17)\n",
    "\n",
    "# Print the first element of each object.\n",
    "print(X_train.head(1))\n",
    "print(X_test.head(1))\n",
    "print(y_train.head(1))\n",
    "print(y_test.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative Approach to Creating `X` and `y`: Patsy\n",
    "\n",
    "We will make one more modification. Patsy is a library that allows you to quickly perform simple data transformations in a style similar to R.\n",
    "\n",
    "Rather than manually creating X and y, we will use the `.dmatricies()` function from Patsy to create the matricies and explore the effect of changing features on training and testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.354343Z",
     "start_time": "2020-10-20T04:49:30.349097Z"
    }
   },
   "outputs": [],
   "source": [
    "# We're not using this today, but you may see it out in the wild so it's here for reference.\n",
    "# patsy.dmatrices(\"MEDV ~ AGE + RM\", data=boston_df, return_type=\"dataframe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split X and y into training and testing sets (using `random_state` for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.369184Z",
     "start_time": "2020-10-20T04:49:30.358017Z"
    }
   },
   "outputs": [],
   "source": [
    "X = boston_df[['AGE', 'RM']]\n",
    "y = boston_df['MEDV']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.389830Z",
     "start_time": "2020-10-20T04:49:30.373923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>38.4</td>\n",
       "      <td>6.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>96.1</td>\n",
       "      <td>6.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>68.8</td>\n",
       "      <td>6.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>68.2</td>\n",
       "      <td>5.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>94.7</td>\n",
       "      <td>5.637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AGE     RM\n",
       "201  38.4  6.162\n",
       "168  96.1  6.319\n",
       "185  68.8  6.153\n",
       "35   68.2  5.933\n",
       "129  94.7  5.637"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.402382Z",
     "start_time": "2020-10-20T04:49:30.391416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test the model on the testing set and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.424121Z",
     "start_time": "2020-10-20T04:49:30.410832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training MSE: 38.969055787686834\n",
      " Testing MSE:  42.001752214868425\n",
      "\n",
      " Training RMSE: 6.242519986967349\n",
      " Testing RMSE:  6.480875883309943\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Training MSe\n",
    "mse_train = mean_squared_error(y_train, lr.predict(X_train))\n",
    "\n",
    "# Testing MSE\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f' Training MSE: {mse_train}')\n",
    "print(f' Testing MSE:  {mse_test}')\n",
    "\n",
    "# Training and TestingRMSE\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "print()\n",
    "print(f' Training RMSE: {rmse_train}')\n",
    "print(f' Testing RMSE:  {rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias-variance tradeoff](./assets/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go back to Step 1 and try adding new variables and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training error**: Decreases as model complexity increases (lower value of k).\n",
    "- **Testing error**: Is minimized at the optimum model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a Model\n",
    "Building and tweaking a model can take a lot of time and might occur over hours or days even. So you don't have to rebuild things from scratch each time you can actually save the current state of your model.\n",
    "\n",
    "`sklearn` offers ways to do this and you can seem some basic guidance at this link: [Model Persistance in sklearn](https://scikit-learn.org/stable/modules/model_persistence.html)\n",
    "\n",
    "**Note**. This can also be used to more extensively save a range of data. You may consider saving your training data and other snapshots of data that let you validate the model build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.439376Z",
     "start_time": "2020-10-20T04:49:30.425570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR RMSE: 6.480875883309943\n",
      "LR Pickle RMSE: 6.480875883309943\n"
     ]
    }
   ],
   "source": [
    "#import pickle\n",
    "\n",
    "# Using pickle\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(lr, open(\"lr_pickle.pkl\", 'wb'))\n",
    "\n",
    "# Restore the model\n",
    "lr_from_pickle = pickle.load(open('lr_pickle.pkl', 'rb'))\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "y_pred_from_pickle = lr_from_pickle.predict(X_test)\n",
    "\n",
    "# Compute Test RMSE for both models\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse_lr_pickle = np.sqrt(mean_squared_error(y_test, y_pred_from_pickle))\n",
    "\n",
    "print(f'LR RMSE: {rmse_lr}')\n",
    "print(f'LR Pickle RMSE: {rmse_lr_pickle}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.458769Z",
     "start_time": "2020-10-20T04:49:30.444563Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dump' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-24ca475b993f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Save the model as a pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'filename.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Load fthe model from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dump' is not defined"
     ]
    }
   ],
   "source": [
    "# Try joblib, which is more efficient at handling large numpy arrays of data.\n",
    "#from joblib import dump, load\n",
    "\n",
    "# Save the model as a pickle file\n",
    "dump(lr, 'filename.pkl')\n",
    "\n",
    "# Load fthe model from the file\n",
    "lr_from_joblib = load('filename.pkl')\n",
    "\n",
    "# Use the model to make predictions\n",
    "y_pred_from_joblib = lr_from_joblib.predict(X_test)\n",
    "\n",
    "# Testing RMSE\n",
    "# Compute Test RMSE for both models\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse_lr_joblib = np.sqrt(mean_squared_error(y_test, y_pred_from_joblib))\n",
    "\n",
    "print(f'LR RMSE: {rmse_lr}')\n",
    "print(f'LR Pickle RMSE: {rmse_lr_joblib}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING: Don't just download and load random pickle files. They aren't secure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-test-performance-with-a-null-baseline\"></a>\n",
    "### Comparing Test Performance With a Null Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When interpreting the predictive power of a model, it's best to compare it to a baseline using a dummy model, sometimes called a ZeroR model or a baseline model. A dummy model is simply using the mean, median, or most common value as the prediction. This forms a benchmark to compare your model against and becomes especially important in classification where your null accuracy might be 95 percent.\n",
    "\n",
    "For example, suppose your dataset is **imbalanced** -- it contains 99% one class and 1% the other class. Then, your baseline accuracy (always guessing the first class) would be 99%. So, if your model is less than 99% accurate, you know it is worse than the baseline. Imbalanced datasets generally must be trained differently (with less of a focus on accuracy) because of this.\n",
    "\n",
    "You can alternatively use simple models to achieve baseline results, for example nearest neighbors or a basic unigram bag of words for text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the baseline mean squared error using a null model.\n",
    "How does this compare to what we achieved with linear regression. Is our model making an actual improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.468420Z",
     "start_time": "2020-10-20T04:49:30.461760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null MSE: 93.1148669975404\n",
      "Null RMSE: 9.649604499539885\n"
     ]
    }
   ],
   "source": [
    "# Brute force - way to build the null model\n",
    "# create an array the same size as y_test and fill with the mean value of y_test\n",
    "y_mean = np.full_like(y_test, y_train.mean())\n",
    "\n",
    "# compute the MSE for the null model, aka the mean\n",
    "mse_null = mean_squared_error(y_test, y_mean)\n",
    "rmse_null = np.sqrt(mse_null)\n",
    "print(f'Null MSE: {mse_null}')\n",
    "print(f'Null RMSE: {rmse_null}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy Models in Scikit-learn\n",
    "Luckily `sklearn` provides an alternative approach to building baseline models, which comes in handy, because it's very much like building any other model. The dummy algorithms are located in the `dummy` module or `sklearn.dummy`.\n",
    "\n",
    "Here we'll use a dummy regessor which we bring in using:\n",
    "\n",
    "`from sklearn.dummy import DummyRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.482199Z",
     "start_time": "2020-10-20T04:49:30.471451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy RMSE: 9.649604499539885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Follow the same steps as a real model\n",
    "# Instantiate\n",
    "dr = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Fit or \"Train\" the dummy regressor\n",
    "dr.fit(X_train, y_train)\n",
    "\n",
    "# save the \"predictions\"\n",
    "y_pred_dummy = dr.predict(X_test)\n",
    "\n",
    "# Test MSE for dummy\n",
    "mse_dum = mean_squared_error(y_test, y_pred_dummy)\n",
    "\n",
    "# Test RMSE for dummy\n",
    "rmse_dum = np.sqrt(mse_dum)\n",
    "\n",
    "print(f'Dummy RMSE: {rmse_dum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise (~20min)\n",
    "Build a linear regression model for the Las Vegas Trip Advisor data.\n",
    "\n",
    "The data is located here: `path = './data/LasVegasTripAdvisorReviews-Dataset.csv'`\n",
    "\n",
    "Carry out the following steps in the cells below:\n",
    "1. Load the data set using `.read_csv()`\n",
    "2. Verify the data was loaded correctly\n",
    "3. Assign `X` and `y` variables using `Hotel Stars` as your feature and `Score` as your target.\n",
    "4. Split the data into Train and Test data sets. Use `random_state=99`.\n",
    "5. Build a linear regression model. Use the standard steps we have demonstrated in previous lessons and in this notebook. You may refer to previous examples in this notebook for reference.\n",
    "6. Compute the RMSE for the training data and testing data.\n",
    "7. Build a baseline model.\n",
    "8. Compute the RMSE for a baseline model using the mean. Remember: here you want to calculate RMSE for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset using `.read_csv()` and the path above. Verify the data loaded properly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.510045Z",
     "start_time": "2020-10-20T04:49:30.484017Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "path = './data/LasVegasTripAdvisorReviews-Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:57:44.385339Z",
     "start_time": "2020-10-20T04:57:44.360350Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick exploration of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:57:58.793252Z",
     "start_time": "2020-10-20T04:57:58.785878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:58:14.418817Z",
     "start_time": "2020-10-20T04:58:14.415875Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at the column names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create X and y using `Hotel Stars` and `Score` as the feature and target, respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.609091Z",
     "start_time": "2020-10-20T04:49:30.601040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign X and y - \n",
    "# Note: Pay attention to the dimensions (aka shape) of your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into train and test data sets. Use `random_state=99`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:58:46.642209Z",
     "start_time": "2020-10-20T04:58:46.638705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test data sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a linear regression model and use it to make predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.638929Z",
     "start_time": "2020-10-20T04:49:30.627004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate the train and test RMSE and print your results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:59:26.327585Z",
     "start_time": "2020-10-20T04:59:26.324712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the train and test RMSE - print your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a baseline model and compute the RMSE for a baseline model using the mean. Print your result.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:59:54.116056Z",
     "start_time": "2020-10-20T04:59:54.100980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a baselne model using the mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the linear regression to the baseline model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-folds-cross-validation\"></a>\n",
    "## K-Folds Cross-Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train/test split provides us with helpful tool, but it's a shame that we are tossing out a large chunk of our data for testing purposes.\n",
    "\n",
    "**How can we use the maximum amount of our data points while still ensuring model integrity?**\n",
    "\n",
    "1. Split our data into a number of different pieces (folds).\n",
    "2. Train using `k-1` folds for training and a different fold for testing.\n",
    "3. Average our model against EACH of those iterations.\n",
    "4. Choose our model and TEST it against the final fold.\n",
    "5. Average all test accuracies to get the estimated out-of-sample accuracy.\n",
    "\n",
    "Although this may sound complicated, we are just training the model on k separate train-test-splits, then taking the average of the resulting test accuracies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"leave-one-out-cross-validation\"></a>\n",
    "### Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A special case of k-fold cross-validation is leave-one-out cross-validation. Rather than taking 510 folds, we take a fold of size `n-1` and leave one observation to test.\n",
    "\n",
    "Typically, 510 fold cross-validation is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro-to-cross-validation-with-the-boston-data\"></a>\n",
    "### Intro to Cross-Validation With the Boston Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a cross-valiation with five folds.\n",
    "`KFold()` provides train/test indices to split data in train/test sets. It splits the dataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining folds form the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.687696Z",
     "start_time": "2020-10-20T04:49:30.676079Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# Build a K-fold with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=5432)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T03:23:49.110295Z",
     "start_time": "2020-10-20T03:23:49.060131Z"
    }
   },
   "source": [
    "**Let's rebuild a model for the boston housing data using `AGE` only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.787411Z",
     "start_time": "2020-10-20T04:49:30.691686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets assign the Boston housing variables again, since we used X & y for other things since then.\n",
    "X = boston_df[['AGE']]\n",
    "y = boston_df['MEDV']\n",
    "\n",
    "# compute the scores - note we're using the linear regression object we instantiated earlier\n",
    "mse_cross = np.mean(-cross_val_score(lr, X, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "rmse_cross = np.sqrt(mse_cross)\n",
    "r_squared_cross = np.mean(cross_val_score(lr, X, y, cv=kf))\n",
    "\n",
    "print(f'Cross Validated MSE: {mse_cross}')\n",
    "print(f'Cross Validated RMSE: {rmse_cross}')\n",
    "print(f'Cross Validated R-squared: {r_squared_cross}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat just using `AGE` and `RM` as the feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.886657Z",
     "start_time": "2020-10-20T04:49:30.796155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets assign the Boston housing variables again, since we used X & y for other things since then.\n",
    "X = boston_df[['AGE', 'RM']]\n",
    "y = boston_df['MEDV']\n",
    "\n",
    "# compute the scores - note we're using the linear regression object we instantiated earlier\n",
    "mse_cross = np.mean(-cross_val_score(lr, X, y, cv=kf, scoring='neg_mean_squared_error'))\n",
    "rmse_cross = np.sqrt(mse_cross)\n",
    "r_squared_cross = np.mean(cross_val_score(lr, X, y, cv=kf))\n",
    "\n",
    "print(f'Cross Validated MSE: {mse_cross}')\n",
    "print(f'Cross Validated RMSE: {rmse_cross}')\n",
    "print(f'Cross Validated R-squared: {r_squared_cross}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look under the hood for a minute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:30.936898Z",
     "start_time": "2020-10-20T04:49:30.893281Z"
    }
   },
   "outputs": [],
   "source": [
    "# run this once as is and then swap kf for an integer between 5 & 10\n",
    "-cross_val_score(lr, X, y, cv=kf, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the range of values above. See how they differ when you substitue an integer for our KFold object, `kf`. Ideally we want our errors to be narrowly distributed. While the average error might be relatively low, it's better if it's low and the range of component errors doesn't vary widely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at the results for each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:31.023240Z",
     "start_time": "2020-10-20T04:49:30.939326Z"
    }
   },
   "outputs": [],
   "source": [
    "mse_values = []\n",
    "scores = []\n",
    "n = 0\n",
    "\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    lr = LinearRegression().fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    \n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lr.predict(X.iloc[test_index])))\n",
    "    scores.append(lr.score(X, y))\n",
    "    \n",
    "    n += 1\n",
    "    \n",
    "    print(f'Model {n}')\n",
    "    print(f'MSE: {mse_values[n-1]}')\n",
    "    print(f'R2: {scores[n-1]}\\n')\n",
    "\n",
    "\n",
    "print(\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print(f'Mean of MSE for all folds: {np.mean(mse_values)}')\n",
    "print(f'Mean of R2 for all folds: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we've found the best model using `AGE` and `RM` we can fit our model using the entire dataset.**\n",
    "\n",
    "Then we would take that final model and put it into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:31.041428Z",
     "start_time": "2020-10-20T04:49:31.029087Z"
    }
   },
   "outputs": [],
   "source": [
    "X = boston_df[['AGE', 'RM']]\n",
    "y = boston_df['MEDV']\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_pred = lr.predict(X)\n",
    "\n",
    "y_pred_series = pd.Series(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:31.056140Z",
     "start_time": "2020-10-20T04:49:31.043235Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_df = pd.concat([y[:10],y_pred_series], axis=1)\n",
    "y_df.columns = ['MEDV', 'y_pred']\n",
    "y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the cross-validated approach here generated more overall error, which of the two approaches would predict new data more accurately  the single model or the cross-validated, averaged one? Why?\n",
    "\n",
    "\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "Use cross validation for the car data set.\n",
    "\n",
    "For this exercise you'll do the following:\n",
    "1. Load the `mtcars.csv` data set. The data is located here: `path = './data/mtcars.csv'`\n",
    "2. Examine the shape of the data and why doing a standard train-test split would be a problem.\n",
    "3. Set up the `X` and `y` using `vs` and `cyl` as features and `mpg` as the target.\n",
    "4. Instantiate a new linear regressor and a 5-fold cross validation on the data. Be sure to shuffle the data and use `random_state=1234`.\n",
    "5. Calculate the cross validated MSE, RMSE, and R-squared. Print your results.\n",
    "6. Repeat steps 2-5 except use `disp` and `qsec` for features. Keep `mpg` as the target.\n",
    "7. Compare the metrics and determine which model will perform better.\n",
    "8. Once you have determined your \"best\" model, apply it to the full dataset and make predictions.\n",
    "9. Build a dataframe to display the first 5 predictions alongside the original data (i.e. the original `y`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the `mtcars.csv` data set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:01:12.853100Z",
     "start_time": "2020-10-20T05:01:12.808307Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the shape of the data and why doing a standard train-test split would be a problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:01:26.517051Z",
     "start_time": "2020-10-20T05:01:26.508281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examine the shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The dataset is very small and a 75/25 split would use only 24 observations to train the model. That's a very small amount of data. It's better to use k-fold cross validation since it will use all the data to give us more robust results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up the `X` and `y` using `vs` and `cyl` as features and `mpg` as the target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:01:44.456193Z",
     "start_time": "2020-10-20T05:01:44.453475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assign the X and y variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate a new linear regressor and a 5-fold cross validation on the data. Be sure to shuffle the data and use `random_state=1234`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T04:49:31.104179Z",
     "start_time": "2020-10-20T04:49:31.099575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up a linear regression and 5-fold cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the cross validated MSE, RMSE, and R-squared. Print your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:06:27.234435Z",
     "start_time": "2020-10-20T05:06:27.225990Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the cross validated MSE, RMSE, and R-squared. Print your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat steps 2-5 except use `disp` and `qsec` for features. Keep `mpg` as the target.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:03:22.032861Z",
     "start_time": "2020-10-20T05:03:22.027852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Repeat steps 2-5 except use disp and qsec for features. Keep mpg as the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the metrics and determine which model will perform better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here- briefly explain why.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once you have determined your \"best\" model, apply it to the full dataset and make predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:11:11.426385Z",
     "start_time": "2020-10-20T05:11:11.423122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a model using the parameters of your best model and fit it to the entire data set and make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a dataframe to display the first 5 predictions alongside the original data (i.e. the original `y`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T05:11:08.843576Z",
     "start_time": "2020-10-20T05:11:08.840004Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build a dataframe of the original y and the predicted y and display the first 5 rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three-way-data-split\"></a>\n",
    "## Three-Way Data Split\n",
    "---\n",
    "\n",
    "The most common workflow is actually a combination of train/test split and cross-validation. We take a train/test split on our data right away and try not spend a lot of time using the testing data set. Instead, we take our training data and tune our models using cross-validation. When we think we are done, we do one last test on the testing data to make sure we haven't accidently overfit to our training data.\n",
    "\n",
    "**If you tune hyperparameters via cross-validation, you should never use cross-validation on the same dataset to estimate OOS accuracy!** Using cross-validation in this way, the entire dataset was used to tune hyperparameters. So, this invalidates our condition above -- where we assumed the test set is a pretend \"out-of-sample\" dataset that was not used to train our model! So, we would expect the accuracy on this test set to be artificially inflated as compared to actual \"out-of-sample\" data.\n",
    "\n",
    "Even with good evaluation procedures, it is incredible easy to overfit our models by including features that will not be available during production or leak information about our testing data in other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/Train-Test-Split-CV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- If model selection and true error estimates are to be computed simultaneously, three disjointed data sets are best.\n",
    "    - **Training set**: A set of examples used for learning  what parameters of the classifier?\n",
    "    - **Validation set**: A set of examples used to tune the parameters of the classifier.\n",
    "    - **Testing set**: A set of examples used ONLY to assess the performance of the fully trained classifier.\n",
    "- Validation and testing must be separate data sets. Once you have the final model set, you cannot do any additional tuning after testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Divide data into training, validation, and testing sets.\n",
    "2. Select architecture (model type) and training parameters (k).\n",
    "3. Train the model using the training set.\n",
    "4. Evaluate the model using the training set.\n",
    "5. Repeat 24 times, selecting different architectures (models) and tuning parameters.\n",
    "6. Select the best model.\n",
    "7. Assess the model with the final testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"additional-resources\"></a>\n",
    "<a id=\"additional-resources\"></a>\n",
    "### Additional Resources\n",
    "- [Bias Variance](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "- University of Washington [slides](https://courses.cs.washington.edu/courses/cse546/12wi/slides/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "### Summary\n",
    "\n",
    "In this lab, we compared four methods of estimating model accuracy on out-of-sample data. Throughout your regular data science work, you will likely use all four at some point:\n",
    "\n",
    "1. **Train on the entire dataset**\n",
    "2. **Train-test-split**\n",
    "3. **Cross-validation**\n",
    "4. **Three-way split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
